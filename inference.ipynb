{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from option.coordinet_option import CoordiNetOption\n",
    "from utils.utils import *\n",
    "from model.coordinet import CoordiNetSystem\n",
    "from dataset.cambridge import CambridgeDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import os, sys\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load image and pose data of scene \"StMarysChurch\", split: train\n",
      "load train data:   0%|          | 0/1487 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load train data: 100%|██████████| 1487/1487 [00:00<00:00, 12220.67it/s]\n",
      "load image and pose data of scene \"StMarysChurch\", split: valid\n",
      "load test data: 100%|██████████| 530/530 [00:00<00:00, 12279.22it/s]\n"
     ]
    }
   ],
   "source": [
    "opt_path='runs/coordinet/church_train_synthesis2/opt.pkl'\n",
    "with open(opt_path, 'rb') as f:\n",
    "    opt = pickle.load(f)\n",
    "# dataset\n",
    "train_dataset = CambridgeDataset(opt.data_root_dir, opt.scene, 'train',320,300)\n",
    "valid_dataset = CambridgeDataset(opt.data_root_dir, opt.scene, 'valid',320,300)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=opt.batch_size, num_workers=16, shuffle=False)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=opt.batch_size, num_workers=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b3\n",
      "load model from runs/coordinet/church_train_synthesis2/ckpt_epoch129_steps1006_angle5.0642_t1.1015.pkl\n"
     ]
    }
   ],
   "source": [
    "# init model\n",
    "ckpt_path = 'runs/coordinet/church_train_synthesis2/ckpt_epoch129_steps1006_angle5.0642_t1.1015.pkl'\n",
    "opt.var_min=[0.25,0.25,0.25,0.0314]\n",
    "coordinet = CoordiNetSystem(opt.feature_dim,opt.W,backbone_path=opt.backbone_path)\n",
    "ckpt = torch.load(ckpt_path)\n",
    "coordinet.load_state_dict(ckpt['coordinet'])\n",
    "print('load model from {}'.format(ckpt_path))\n",
    "coordinet = coordinet.cuda()\n",
    "coordinet.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:08<00:00,  6.31it/s]\n",
      "[valid] angle_diff:5.0642°, t_diff:1.1015m\n"
     ]
    }
   ],
   "source": [
    "# valid: center crop from test set\n",
    "t_diffs = []\n",
    "angle_diffs = []\n",
    "ts=[]\n",
    "for imgs, poses in tqdm(valid_dataloader, total=len(valid_dataloader), file=sys.stdout):\n",
    "    imgs = imgs.cuda()\n",
    "    poses = poses.cuda()\n",
    "    with torch.no_grad():\n",
    "        t, q, _, losses = coordinet.forward(imgs, cal_loss=True, y_pose=poses)\n",
    "    ts.append(t.cpu().numpy())\n",
    "    t_diffs.append(losses['t_diff'].cpu().numpy())\n",
    "    angle_diffs.append(losses['angle_diff'].cpu().numpy())\n",
    "t_diffs = np.hstack(t_diffs)\n",
    "angle_diffs = np.hstack(angle_diffs) / np.pi * 180\n",
    "angle_diff = np.mean(angle_diffs)\n",
    "t_diff = np.mean(np.hstack(t_diffs))\n",
    "print('[valid] angle_diff:%.4f°, t_diff:%.4fm' % (angle_diff, t_diff))\n",
    "\n",
    "ts=np.vstack(ts)\n",
    "# save\n",
    "with open('./valid_diff.txt', 'w') as f:\n",
    "    for i in range(len(t_diffs)):\n",
    "        GT_t=valid_dataset.test_poses[i][:3,3]\n",
    "        t=ts[i]\n",
    "        f.write('%.4fm %.4f° [GT:(%.3f,%.3f,%.3f)] [Re:(%.3f,%.3f,%.3f)]\\n' % (t_diffs[i], angle_diffs[i],*list(GT_t),*list(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [04:56<00:00,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[test] angle_diff:5.8118°, t_diff:1.1466m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# test: uniform num crops from test set\n",
    "t_diffs = []\n",
    "angle_diffs = []\n",
    "ts=[]\n",
    "num_crops=128\n",
    "for i in tqdm(range(len(valid_dataset.test_paths))):\n",
    "    imgs,poses=valid_dataset.uniform_crop(valid_dataset.test_paths[i],num_crops,valid_dataset.test_poses[i])\n",
    "    it,it_diffs,iangle_diffs=[],[],[]\n",
    "    for j in range(0,len(imgs),opt.batch_size):\n",
    "        img,pose=imgs[j:j+opt.batch_size],poses[j:j+opt.batch_size]\n",
    "        img,pose=img.cuda(),pose.cuda()\n",
    "        with torch.no_grad():\n",
    "            t, q, _, losses = coordinet.forward(img, cal_loss=True, y_pose=pose)\n",
    "        it.append(t.cpu().numpy())\n",
    "        it_diffs.append(losses['t_diff'].cpu().numpy())\n",
    "        iangle_diffs.append(losses['angle_diff'].cpu().numpy())\n",
    "    it = np.vstack(it)\n",
    "    it_diffs = np.hstack(it_diffs)\n",
    "    iangle_diffs = np.hstack(iangle_diffs) / np.pi * 180\n",
    "    ts.append(np.mean(it,axis=0))\n",
    "    t_diffs.append(np.mean(it_diffs))\n",
    "    angle_diffs.append(np.mean(iangle_diffs))\n",
    "t_diffs = np.hstack(t_diffs)\n",
    "angle_diffs = np.hstack(angle_diffs)\n",
    "t_diff=np.mean(t_diffs)\n",
    "angle_diff=np.mean(angle_diffs)\n",
    "print('[test] angle_diff:%.4f°, t_diff:%.4fm' % (angle_diff, t_diff))\n",
    "ts=np.vstack(ts)\n",
    "# save\n",
    "with open('./test_diff.txt', 'w') as f:\n",
    "    for i in range(len(t_diffs)):\n",
    "        GT_t=valid_dataset.test_poses[i][:3,3]\n",
    "        t=ts[i]\n",
    "        f.write('%.4fm %.4f° [GT:(%.3f,%.3f,%.3f)] [Re:(%.3f,%.3f,%.3f)]\\n' % (t_diffs[i], angle_diffs[i],*list(GT_t),*list(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [00:17<00:00,  8.34it/s]\n",
      "[train] angle_diff:3.2010°, t_diff:0.6388m\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "coordinet.eval()\n",
    "t_diffs = []\n",
    "angle_diffs = []\n",
    "for imgs, poses in tqdm(train_dataloader, total=len(train_dataloader), file=sys.stdout):\n",
    "    imgs = imgs.cuda()\n",
    "    poses = poses.cuda()\n",
    "    with torch.no_grad():\n",
    "        t, q, _, losses = coordinet.forward(imgs, cal_loss=True, y_pose=poses)\n",
    "    t_diffs.append(losses['t_diff'].cpu().numpy())\n",
    "    angle_diffs.append(losses['angle_diff'].cpu().numpy())\n",
    "t_diffs = np.hstack(t_diffs)\n",
    "angle_diffs = np.hstack(angle_diffs) / np.pi * 180\n",
    "angle_diff = np.mean(angle_diffs)\n",
    "t_diff = np.mean(t_diffs)\n",
    "print('[train] angle_diff:%.4f°, t_diff:%.4fm' % (angle_diff, t_diff))\n",
    "# save\n",
    "with open('./train_diff.txt', 'w') as f:\n",
    "    for i in range(len(t_diffs)):\n",
    "        f.write('%.4fm %.4f°\\n' % (t_diffs[i], angle_diffs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
